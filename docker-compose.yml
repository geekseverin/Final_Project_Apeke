services:
  # Master Node (NameNode, ResourceManager, Spark Master)
  hadoop-master:
    build: 
      context: ./dockerfiles/hadoop-master
    container_name: hadoop-master
    hostname: hadoop-master
    ports:
      - "9870:9870"  # HDFS Web UI
      - "8088:8088"  # Yarn Web UI
      - "8080:8080"  # Spark Web UI
      - "7077:7077"  # Spark Master Port
      - "4040:4040"  # Spark Application UI
    volumes:
      - ./config:/config
      - ./scripts:/scripts
      - hadoop_master_data:/hadoop/data
    environment:
      - HADOOP_ROLE=master
      - CLUSTER_NAME=bigdata-cluster
      - JAVA_HOME=/usr/local/openjdk-8
      - PYTHONPATH=/opt/spark/python:/opt/spark/python/lib/py4j-0.10.9.7-src.zip
      - PYSPARK_PYTHON=python3
    networks:
      - bigdata-net
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9870"]
      interval: 30s
      timeout: 10s
      retries: 3
    command: ["/scripts/setup/init-hadoop.sh", "master"]

  # Secondary Master Node (SecondaryNameNode)
  hadoop-secondary:
    build: 
      context: ./dockerfiles/hadoop-base
    container_name: hadoop-secondary
    hostname: hadoop-secondary
    ports:
      - "9868:9868"  # Secondary NameNode Web UI
    volumes:
      - ./config:/config
      - ./scripts:/scripts
      - hadoop_secondary_data:/hadoop/data
    environment:
      - HADOOP_ROLE=secondary
      - CLUSTER_NAME=bigdata-cluster
      - JAVA_HOME=/usr/local/openjdk-8
    networks:
      - bigdata-net
    depends_on:
      - hadoop-master
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9868"]
      interval: 30s
      timeout: 10s
      retries: 3
    command: ["/scripts/setup/init-hadoop.sh", "secondary"]

  # Worker Node 1
  hadoop-worker1:
    build: 
      context: ./dockerfiles/hadoop-worker
    container_name: hadoop-worker1
    hostname: hadoop-worker1
    ports:
      - "8081:8081"  # Spark Worker Web UI
    volumes:
      - ./config:/config
      - ./scripts:/scripts
      - hadoop_worker1_data:/hadoop/data
    environment:
      - HADOOP_ROLE=worker
      - WORKER_ID=1
      - CLUSTER_NAME=bigdata-cluster
      - JAVA_HOME=/usr/local/openjdk-8
      - PYTHONPATH=/opt/spark/python:/opt/spark/python/lib/py4j-0.10.9.7-src.zip
      - PYSPARK_PYTHON=python3
    networks:
      - bigdata-net
    depends_on:
      - hadoop-master
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081"]
      interval: 30s
      timeout: 10s
      retries: 3
    command: ["/scripts/setup/init-hadoop.sh", "worker"]

  # Worker Node 2
  hadoop-worker2:
    build: 
      context: ./dockerfiles/hadoop-worker
    container_name: hadoop-worker2
    hostname: hadoop-worker2
    ports:
      - "8082:8082"  # Spark Worker Web UI
    volumes:
      - ./config:/config
      - ./scripts:/scripts
      - hadoop_worker2_data:/hadoop/data
    environment:
      - HADOOP_ROLE=worker
      - WORKER_ID=2
      - CLUSTER_NAME=bigdata-cluster
      - JAVA_HOME=/usr/local/openjdk-8
      - PYTHONPATH=/opt/spark/python:/opt/spark/python/lib/py4j-0.10.9.7-src.zip
      - PYSPARK_PYTHON=python3
    networks:
      - bigdata-net
    depends_on:
      - hadoop-master
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8082"]
      interval: 30s
      timeout: 10s
      retries: 3
    command: ["/scripts/setup/init-hadoop.sh", "worker"]

  # Worker Node 3
  hadoop-worker3:
    build: 
      context: ./dockerfiles/hadoop-worker
    container_name: hadoop-worker3
    hostname: hadoop-worker3
    ports:
      - "8083:8083"  # Spark Worker Web UI
    volumes:
      - ./config:/config
      - ./scripts:/scripts
      - hadoop_worker3_data:/hadoop/data
    environment:
      - HADOOP_ROLE=worker
      - WORKER_ID=3
      - CLUSTER_NAME=bigdata-cluster
      - JAVA_HOME=/usr/local/openjdk-8
      - PYTHONPATH=/opt/spark/python:/opt/spark/python/lib/py4j-0.10.9.7-src.zip
      - PYSPARK_PYTHON=python3
    networks:
      - bigdata-net
    depends_on:
      - hadoop-master
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8083"]
      interval: 30s
      timeout: 10s
      retries: 3
    command: ["/scripts/setup/init-hadoop.sh", "worker"]

  # MongoDB
  mongodb:
    build: 
      context: ./dockerfiles/mongodb
    container_name: mongodb
    hostname: mongodb
    ports:
      - "27017:27017"
    volumes:
      - mongodb_data:/data/db
      - ./scripts:/scripts
      - ./dockerfiles/mongodb/sample-data:/sample-data
    environment:
      - MONGO_INITDB_ROOT_USERNAME=admin
      - MONGO_INITDB_ROOT_PASSWORD=password123
      - MONGO_INITDB_DATABASE=bigdata
    networks:
      - bigdata-net
    healthcheck:
      test: ["CMD", "mongosh", "--host", "localhost:27017", "--eval", "db.runCommand('ping')"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  # Application Web Flask
  web-app:
    build: 
      context: ./app
    container_name: web-app
    hostname: web-app
    ports:
      - "5000:5000"
    volumes:
      - ./app:/app
    environment:
      - FLASK_ENV=development
      - FLASK_DEBUG=1
      - MONGODB_URI=mongodb://admin:password123@mongodb:27017/bigdata?authSource=admin
      - HADOOP_MASTER=hadoop-master
    networks:
      - bigdata-net
    depends_on:
      - hadoop-master
      - mongodb
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

volumes:
  hadoop_master_data:
    driver: local
  hadoop_secondary_data:
    driver: local
  hadoop_worker1_data:
    driver: local
  hadoop_worker2_data:
    driver: local
  hadoop_worker3_data:
    driver: local
  mongodb_data:
    driver: local

networks:
  bigdata-net:
    driver: bridge
    ipam:
      config:
        - subnet: 172.25.0.0/16