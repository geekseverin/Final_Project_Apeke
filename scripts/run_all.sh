#!/bin/bash
# Script principal pour initialiser et d√©marrer le cluster Big Data
# Projet Big Data - Traitement Distribu√© 2024-2025

set -e

echo "üöÄ D√©marrage du cluster Big Data..."

# Couleurs pour l'affichage
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# Fonction pour attendre qu'un service soit pr√™t
wait_for_service() {
    local host=$1
    local port=$2
    local service_name=$3
    local max_attempts=30
    local attempt=1
    
    log_info "Attente du d√©marrage de $service_name sur $host:$port..."
    
    while [ $attempt -le $max_attempts ]; do
        if docker exec hadoop-master nc -z $host $port 2>/dev/null; then
            log_success "$service_name est pr√™t!"
            return 0
        fi
        
        echo -n "."
        sleep 2
        attempt=$((attempt + 1))
    done
    
    log_error "Timeout: $service_name n'a pas d√©marr√© dans les temps"
    return 1
}

# 1. V√©rifier que Docker Compose est en cours d'ex√©cution
log_info "V√©rification des conteneurs..."
if ! docker-compose ps | grep -q "Up"; then
    log_error "Les conteneurs ne sont pas d√©marr√©s. Lancez 'docker-compose up -d' d'abord."
    exit 1
fi

# 2. Attendre que les services critiques soient pr√™ts
wait_for_service hadoop-master 9870 "Hadoop NameNode"
wait_for_service hadoop-master 8088 "Yarn ResourceManager"
wait_for_service hadoop-master 7077 "Spark Master"
wait_for_service mongodb 27017 "MongoDB"

# 3. Initialiser les donn√©es MongoDB
log_info "Initialisation des donn√©es MongoDB..."
docker exec mongodb /scripts/setup/populate_mongodb.sh 2>/dev/null || log_warning "MongoDB d√©j√† initialis√©"

# 4. Cr√©er les r√©pertoires HDFS n√©cessaires
log_info "Cr√©ation des r√©pertoires HDFS..."
docker exec hadoop-master hdfs dfs -mkdir -p /data || true
docker exec hadoop-master hdfs dfs -mkdir -p /pig-output || true
docker exec hadoop-master hdfs dfs -mkdir -p /spark-output || true
docker exec hadoop-master hdfs dfs -chmod 777 /data || true
docker exec hadoop-master hdfs dfs -chmod 777 /pig-output || true
docker exec hadoop-master hdfs dfs -chmod 777 /spark-output || true
log_success "R√©pertoires HDFS cr√©√©s"

# 5. Cr√©er des donn√©es CSV simples directement dans HDFS (solution alternative)
log_info "Cr√©ation des donn√©es CSV pour Pig..."

# Cr√©er sales.csv directement
docker exec hadoop-master bash -c 'cat > /tmp/sales.csv << EOF
id,product,quantity,price,date,customer_id
s001,Laptop Dell XPS,1,1299.99,2024-01-15,c001
s002,iPhone 15,2,999.99,2024-01-16,c002
s003,Samsung Galaxy S24,1,899.99,2024-01-17,c003
s004,MacBook Pro,1,2299.99,2024-01-18,c004
s005,iPad Air,3,649.99,2024-01-19,c005
s006,AirPods Pro,2,279.99,2024-01-20,c001
s007,Laptop Dell XPS,1,1299.99,2024-01-21,c006
s008,Samsung Galaxy S24,2,899.99,2024-01-22,c007
s009,iPhone 15,1,999.99,2024-01-23,c008
s010,MacBook Pro,1,2299.99,2024-01-24,c009
s011,AirPods Pro,4,279.99,2024-01-25,c010
s012,iPad Air,1,649.99,2024-01-26,c002
s013,Laptop Dell XPS,2,1299.99,2024-01-27,c003
s014,iPhone 15,1,999.99,2024-01-28,c011
s015,Samsung Galaxy S24,3,899.99,2024-01-29,c012
EOF'

# Cr√©er customers.csv directement
docker exec hadoop-master bash -c 'cat > /tmp/customers.csv << EOF
id,name,email,city,age
c001,Alice Martin,alice.martin@email.com,Paris,28
c002,Bob Dupont,bob.dupont@email.com,Lyon,35
c003,Claire Moreau,claire.moreau@email.com,Marseille,42
c004,David Bernard,david.bernard@email.com,Paris,31
c005,Emma Leroy,emma.leroy@email.com,Toulouse,26
c006,Fran√ßois Petit,francois.petit@email.com,Nice,39
c007,Gabrielle Roux,gabrielle.roux@email.com,Lyon,33
c008,Henri Blanc,henri.blanc@email.com,Strasbourg,45
c009,Isabelle Girard,isabelle.girard@email.com,Paris,29
c010,Julien Morel,julien.morel@email.com,Bordeaux,37
c011,Karine Fournier,karine.fournier@email.com,Marseille,41
c012,Laurent Simon,laurent.simon@email.com,Toulouse,34
EOF'

# Copier vers HDFS
docker exec hadoop-master hdfs dfs -put -f /tmp/sales.csv /data/
docker exec hadoop-master hdfs dfs -put -f /tmp/customers.csv /data/

# Nettoyer les fichiers temporaires
docker exec hadoop-master rm -f /tmp/sales.csv /tmp/customers.csv

log_success "Donn√©es CSV cr√©√©es et copi√©es vers HDFS"

# V√©rifier les donn√©es dans HDFS
log_info "V√©rification des donn√©es dans HDFS..."
docker exec hadoop-master hdfs dfs -ls /data/
docker exec hadoop-master hdfs dfs -head /data/sales.csv

# 6. Ex√©cuter l'analyse Pig
log_info "Ex√©cution de l'analyse Apache Pig..."
docker exec hadoop-master pig -f /scripts/pig/data_analysis.pig 2>/dev/null
if [ $? -eq 0 ]; then
    log_success "Analyse Pig termin√©e avec succ√®s"
else
    log_warning "Erreur lors de l'analyse Pig - v√©rifiez les logs"
    # Afficher les logs pour debug
    docker exec hadoop-master tail -20 /home/hadoop/pig_*.log 2>/dev/null || true
fi

# 7. Ex√©cuter l'analyse Spark (optionnel)
log_info "Tentative d'ex√©cution de l'analyse Apache Spark..."
if docker exec hadoop-master test -f /scripts/spark/mongodb_reader.py; then
    docker exec hadoop-master spark-submit \
        --jars /opt/hadoop/share/hadoop/common/lib/mongo-hadoop-core-2.0.2.jar,/opt/hadoop/share/hadoop/common/lib/mongodb-driver-3.12.11.jar \
        --conf "spark.mongodb.input.uri=mongodb://admin:password123@mongodb:27017/bigdata.sales" \
        --conf "spark.mongodb.output.uri=mongodb://admin:password123@mongodb:27017/bigdata.results" \
        /scripts/spark/mongodb_reader.py 2>/dev/null
    if [ $? -eq 0 ]; then
        log_success "Analyse Spark termin√©e avec succ√®s"
    else
        log_warning "Erreur lors de l'analyse Spark"
    fi
else
    log_warning "Script Spark non trouv√© - ignor√©"
fi

# 8. V√©rifier l'√©tat du cluster et les r√©sultats
log_info "V√©rification des r√©sultats d'analyse..."
echo ""
echo "=== R√âSULTATS DES ANALYSES ==="

# V√©rifier les r√©sultats Pig
if docker exec hadoop-master hdfs dfs -test -d /pig-output/product-analysis 2>/dev/null; then
    echo "‚úÖ Analyse Pig des produits: Termin√©e"
    echo "Aper√ßu des r√©sultats:"
    docker exec hadoop-master hdfs dfs -head /pig-output/product-analysis/part-r-00000 2>/dev/null || true
else
    echo "‚ùå Analyse Pig des produits: Non trouv√©e"
fi

if docker exec hadoop-master hdfs dfs -test -d /pig-output/city-revenue 2>/dev/null; then
    echo "‚úÖ Analyse Pig des villes: Termin√©e"
    echo "Aper√ßu des r√©sultats:"
    docker exec hadoop-master hdfs dfs -head /pig-output/city-revenue/part-r-00000 2>/dev/null || true
else
    echo "‚ùå Analyse Pig des villes: Non trouv√©e"
fi

echo ""
echo "=== √âTAT DU CLUSTER ==="
echo "üåê Hadoop NameNode: http://localhost:9870"
echo "‚öôÔ∏è  Yarn ResourceManager: http://localhost:8088"
echo "‚ö° Spark Master: http://localhost:8080"
echo "üìä Dashboard Web: http://localhost:5000"
echo "üçÉ MongoDB: mongodb://localhost:27017"
echo ""

# Test de connectivit√© pour les services web
echo "=== TEST DE CONNECTIVIT√â ==="
test_url() {
    local url=$1
    local name=$2
    if curl -s -f "$url" > /dev/null 2>&1; then
        echo "‚úÖ $name: Accessible"
    else
        echo "‚ùå $name: Non accessible - V√©rifiez que le port est expos√©"
    fi
}

test_url "http://localhost:9870" "Hadoop NameNode"
test_url "http://localhost:8088" "Yarn ResourceManager"  
test_url "http://localhost:8080" "Spark Master"
test_url "http://localhost:5000" "Dashboard Web"

# Afficher un r√©sum√© des services
echo ""
echo "=== STATUT DES CONTENEURS ==="
docker-compose ps

echo ""
log_success "üéâ Cluster Big Data configur√©!"
echo ""
echo "Si les services web ne sont pas accessibles, v√©rifiez:"
echo "  1. Les ports sont bien expos√©s dans docker-compose.yml"
echo "  2. Les services sont d√©marr√©s: docker-compose logs [service-name]"
echo "  3. Red√©marrez si n√©cessaire: docker-compose restart"
echo ""
echo "Pour arr√™ter le cluster: docker-compose down -v"