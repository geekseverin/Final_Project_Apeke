#!/usr/bin/env python3
"""
Script Spark pour lire des donnÃ©es depuis HDFS et les analyser
Projet Big Data - Traitement DistribuÃ© 2024-2025
VERSION CORRIGÃ‰E
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import sys

# CORRECTION : Import conditionnel de pymongo
try:
    import pymongo
    PYMONGO_AVAILABLE = True
    print("pymongo disponible - sauvegarde MongoDB activÃ©e")
except ImportError:
    PYMONGO_AVAILABLE = False
    print("pymongo non disponible - sauvegarde MongoDB dÃ©sactivÃ©e")

def create_spark_session():
    """CrÃ©er une session Spark optimisÃ©e"""
    return SparkSession.builder \
        .appName("BigData-HDFS-Analysis") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .config("spark.executor.memory", "1g") \
        .config("spark.driver.memory", "1g") \
        .getOrCreate()

def read_hdfs_data(spark):
    """Lire les donnÃ©es depuis HDFS avec gestion d'erreurs amÃ©liorÃ©e"""
    print("=== ğŸ“Š Lecture des donnÃ©es HDFS ===")
    
    try:
        # VÃ©rifier la connectivitÃ© HDFS
        sc = spark.sparkContext
        hadoop_conf = sc._jsc.hadoopConfiguration()
        
        # Lire les donnÃ©es de ventes depuis HDFS
        print("ğŸ“‹ Lecture des donnÃ©es de ventes...")
        sales_path = "hdfs://hadoop-master:9000/data/sales.json"
        sales_df = spark.read.option("multiline", "true").json(sales_path)
        
        if sales_df.count() == 0:
            print("âŒ Aucune donnÃ©e de vente trouvÃ©e")
            return None, None
            
        print(f"âœ… Ventes chargÃ©es: {sales_df.count()} enregistrements")
        print("ğŸ” Structure des ventes:")
        sales_df.printSchema()
        sales_df.show(3, truncate=False)
        
        # Lire les donnÃ©es clients depuis HDFS
        print("\nğŸ‘¥ Lecture des donnÃ©es clients...")
        customers_path = "hdfs://hadoop-master:9000/data/customers.json"
        customers_df = spark.read.option("multiline", "true").json(customers_path)
        
        if customers_df.count() == 0:
            print("âŒ Aucune donnÃ©e client trouvÃ©e")
            return None, None
            
        print(f"âœ… Clients chargÃ©s: {customers_df.count()} enregistrements")
        print("ğŸ” Structure des clients:")
        customers_df.printSchema()
        customers_df.show(3, truncate=False)
        
        return sales_df, customers_df
        
    except Exception as e:
        print(f"âŒ Erreur lecture HDFS: {str(e)}")
        print("ğŸ”§ VÃ©rifiez que HDFS est dÃ©marrÃ© et que les donnÃ©es existent")
        return None, None

def validate_and_clean_data(sales_df, customers_df):
    """Nettoyer et valider les donnÃ©es"""
    print("\n=== ğŸ§¹ Nettoyage et validation des donnÃ©es ===")
    
    # Nettoyer les donnÃ©es de ventes
    sales_clean = sales_df.filter(
        (col("price").isNotNull()) & 
        (col("quantity").isNotNull()) & 
        (col("price") > 0) & 
        (col("quantity") > 0) &
        (col("product").isNotNull()) &
        (col("customer_id").isNotNull())
    )
    
    # Nettoyer les donnÃ©es clients
    customers_clean = customers_df.filter(
        (col("id").isNotNull()) &
        (col("city").isNotNull()) &
        (col("name").isNotNull())
    )
    
    print(f"ğŸ“Š Ventes valides: {sales_clean.count()}/{sales_df.count()}")
    print(f"ğŸ‘¥ Clients valides: {customers_clean.count()}/{customers_df.count()}")
    
    return sales_clean, customers_clean

def analyze_data(sales_df, customers_df):
    """Effectuer des analyses complÃ¨tes sur les donnÃ©es"""
    print("\n=== ğŸ“ˆ Analyse des donnÃ©es avec Spark ===")
    
    if sales_df is None or customers_df is None:
        print("âŒ Pas de donnÃ©es Ã  analyser")
        return {}
    
    # Nettoyer les donnÃ©es
    sales_clean, customers_clean = validate_and_clean_data(sales_df, customers_df)
    
    # 1. Calcul du montant total par vente
    print("\nğŸ’° Calcul des montants totaux...")
    sales_with_total = sales_clean.withColumn("total_amount", 
                                             col("quantity").cast("double") * col("price").cast("double"))
    
    # 2. Analyse des ventes par produit
    print("ğŸ›ï¸ Analyse des produits...")
    product_analysis = sales_with_total.groupBy("product") \
        .agg(
            count("*").alias("total_sales"),
            sum("quantity").alias("total_quantity"),
            avg("price").alias("avg_price"),
            sum("total_amount").alias("total_revenue")
        ) \
        .orderBy(desc("total_revenue"))
    
    print("ğŸ† Top 5 produits par revenus:")
    product_analysis.show(5, truncate=False)
    
    # 3. Jointure clients-ventes pour analyse gÃ©ographique
    print("\nğŸ”— Jointure clients-ventes...")
    sales_customers = sales_with_total.join(
        customers_clean, 
        sales_with_total.customer_id == customers_clean.id,
        "inner"
    )
    
    join_count = sales_customers.count()
    print(f"âœ… Jointure rÃ©ussie: {join_count} enregistrements")
    
    if join_count > 0:
        sales_customers.show(3, truncate=False)
    
    # 4. Analyse par ville
    print("\nğŸŒ† Analyse par ville...")
    city_analysis = sales_customers.groupBy("city") \
        .agg(
            count("*").alias("total_transactions"),
            sum("total_amount").alias("city_revenue"),
            countDistinct("customer_id").alias("unique_customers")
        ) \
        .orderBy(desc("city_revenue"))
    
    print("ğŸ™ï¸ Revenus par ville:")
    city_analysis.show(truncate=False)
    
    # 5. Top clients
    print("\nğŸ‘‘ Analyse des meilleurs clients...")
    customer_analysis = sales_with_total.groupBy("customer_id") \
        .agg(
            count("*").alias("purchase_count"),
            sum("total_amount").alias("customer_total")
        ) \
        .orderBy(desc("customer_total"))
    
    print("ğŸ¥‡ Top 5 clients:")
    customer_analysis.show(5, truncate=False)
    
    # 6. Statistiques gÃ©nÃ©rales
    print("\nğŸ“Š Statistiques gÃ©nÃ©rales:")
    total_revenue = sales_with_total.agg(sum("total_amount")).collect()[0][0]
    total_sales = sales_with_total.count()
    unique_products = sales_with_total.select("product").distinct().count()
    unique_customers = sales_with_total.select("customer_id").distinct().count()
    
    print(f"ğŸ’µ Chiffre d'affaires total: {total_revenue:.2f}â‚¬")
    print(f"ğŸ“¦ Nombre total de ventes: {total_sales}")
    print(f"ğŸ›ï¸ Produits uniques: {unique_products}")
    print(f"ğŸ‘¥ Clients uniques: {unique_customers}")
    
    return {
        'product_analysis': product_analysis,
        'city_analysis': city_analysis,
        'customer_analysis': customer_analysis,
        'sales_with_total': sales_with_total
    }

def save_results_to_hdfs(results, spark):
    """Sauvegarder les rÃ©sultats dans HDFS avec gestion d'erreurs"""
    print("\n=== ğŸ’¾ Sauvegarde des rÃ©sultats dans HDFS ===")
    
    try:
        base_path = "hdfs://hadoop-master:9000/spark-output"
        
        # Sauvegarder l'analyse des produits
        print("ğŸ“ Sauvegarde analyse produits...")
        results['product_analysis'].coalesce(1) \
            .write \
            .mode("overwrite") \
            .option("header", "true") \
            .csv(f"{base_path}/product-analysis")
        
        # Sauvegarder l'analyse des villes
        print("ğŸŒ Sauvegarde analyse villes...")
        results['city_analysis'].coalesce(1) \
            .write \
            .mode("overwrite") \
            .option("header", "true") \
            .csv(f"{base_path}/city-analysis")
        
        # Sauvegarder l'analyse des clients
        print("ğŸ‘¤ Sauvegarde analyse clients...")
        results['customer_analysis'].coalesce(1) \
            .write \
            .mode("overwrite") \
            .option("header", "true") \
            .csv(f"{base_path}/customer-analysis")
        
        print("âœ… RÃ©sultats sauvegardÃ©s dans HDFS avec succÃ¨s!")
        
        # VÃ©rifier les fichiers crÃ©Ã©s
        print("\nğŸ“ Fichiers crÃ©Ã©s dans HDFS:")
        import subprocess
        subprocess.run(["hdfs", "dfs", "-ls", "-R", "/spark-output"], check=False)
        
        return True
        
    except Exception as e:
        print(f"âŒ Erreur sauvegarde HDFS: {str(e)}")
        return False

def save_results_to_mongodb(results, spark):
    """Sauvegarder les rÃ©sultats dans MongoDB si pymongo est disponible"""
    if not PYMONGO_AVAILABLE:
        print("âš ï¸ pymongo non disponible - sauvegarde MongoDB ignorÃ©e")
        return False
        
    print("\n=== ğŸƒ Sauvegarde des rÃ©sultats dans MongoDB ===")
    
    try:
        # Connexion MongoDB avec timeout
        client = pymongo.MongoClient(
            "mongodb://admin:password123@mongodb:27017/?authSource=admin",
            serverSelectionTimeoutMS=5000
        )
        
        # Test de connexion
        client.admin.command('ping')
        db = client.bigdata
        
        # Convertir et sauvegarder l'analyse des produits
        print("ğŸ“Š Sauvegarde analyse produits...")
        product_data = results['product_analysis'].collect()
        
        if product_data:
            product_docs = []
            for row in product_data:
                product_docs.append({
                    'product': row['product'],
                    'total_sales': int(row['total_sales']),
                    'total_quantity': int(row['total_quantity']) if row['total_quantity'] else 0,
                    'avg_price': float(row['avg_price']) if row['avg_price'] else 0.0,
                    'total_revenue': float(row['total_revenue']) if row['total_revenue'] else 0.0,
                    'analysis_date': '2024-09-02'
                })
            
            # Supprimer et insÃ©rer
            db.product_analysis.drop()
            db.product_analysis.insert_many(product_docs)
            print(f"âœ… SauvegardÃ© {len(product_docs)} analyses de produits dans MongoDB")
        
        # Sauvegarder analyse des villes
        print("ğŸŒ Sauvegarde analyse villes...")
        city_data = results['city_analysis'].collect()
        
        if city_data:
            city_docs = []
            for row in city_data:
                city_docs.append({
                    'city': row['city'],
                    'total_transactions': int(row['total_transactions']),
                    'city_revenue': float(row['city_revenue']) if row['city_revenue'] else 0.0,
                    'unique_customers': int(row['unique_customers']) if row['unique_customers'] else 0,
                    'analysis_date': '2024-09-02'
                })
            
            db.city_analysis.drop()
            db.city_analysis.insert_many(city_docs)
            print(f"âœ… SauvegardÃ© {len(city_docs)} analyses de villes dans MongoDB")
        
        client.close()
        print("âœ… RÃ©sultats sauvegardÃ©s dans MongoDB avec succÃ¨s!")
        return True
        
    except Exception as e:
        print(f"âŒ Erreur sauvegarde MongoDB: {str(e)}")
        return False

def main():
    """Fonction principale avec gestion d'erreurs complÃ¨te"""
    print("ğŸš€ DÃ©marrage de l'analyse Big Data avec Spark...")
    print("=" * 60)
    
    # CrÃ©er la session Spark
    spark = create_spark_session()
    spark.sparkContext.setLogLevel("WARN")
    
    try:
        print(f"âœ… Session Spark crÃ©Ã©e: {spark.version}")
        print(f"ğŸ¯ Application: {spark.sparkContext.appName}")
        
        # Lire les donnÃ©es depuis HDFS
        sales_df, customers_df = read_hdfs_data(spark)
        
        if sales_df is not None and customers_df is not None:
            # Analyser les donnÃ©es
            results = analyze_data(sales_df, customers_df)
            
            if results:
                # Sauvegarder les rÃ©sultats
                hdfs_success = save_results_to_hdfs(results, spark)
                mongo_success = save_results_to_mongodb(results, spark)
                
                print("\n" + "=" * 60)
                print("ğŸ“Š RÃ‰SUMÃ‰ DE L'ANALYSE")
                print("=" * 60)
                print(f"ğŸ’¾ Sauvegarde HDFS: {'âœ… RÃ©ussie' if hdfs_success else 'âŒ Ã‰chec'}")
                print(f"ğŸƒ Sauvegarde MongoDB: {'âœ… RÃ©ussie' if mongo_success else 'âŒ Ã‰chec'}")
                print("âœ… Analyse terminÃ©e avec succÃ¨s!")
            else:
                print("âŒ Aucun rÃ©sultat d'analyse gÃ©nÃ©rÃ©")
        else:
            print("âŒ Impossible de charger les donnÃ©es")
            sys.exit(1)
        
    except Exception as e:
        print(f"âŒ Erreur critique lors de l'analyse: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    
    finally:
        print("\nğŸ”š ArrÃªt de la session Spark...")
        spark.stop()
        print("âœ… Session fermÃ©e")

if __name__ == "__main__":
    main()